#!/usr/bin/env python3
"""
Advanced Web Spider - Ø²Ø§Ø­Ù ÙˆÙŠØ¨ Ù…ØªÙ‚Ø¯Ù… ÙŠØ¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ù…ÙˆØ§Ù‚Ø¹ Ø­Ù‚ÙŠÙ‚ÙŠØ©
"""

import requests
import sqlite3
import json
import os
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
from datetime import datetime

class AdvancedWebSpider:
    def __init__(self):
        self.knowledge_db = "data/knowledge/knowledge.db"
        self.visited_urls = set()
        self.setup_database()
        
    def setup_database(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        os.makedirs("data/knowledge", exist_ok=True)
        conn = sqlite3.connect(self.knowledge_db)
        cursor = conn.cursor()
        
        # Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ø§Ù„ÙˆÙŠØ¨
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS web_knowledge (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT,
                content TEXT,
                url TEXT UNIQUE,
                source_type TEXT,
                category TEXT,
                difficulty TEXT,
                tags TEXT,
                content_length INTEGER,
                crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                quality_score REAL DEFAULT 0.0
            )
        ''')
        
        # Ø¬Ø¯ÙˆÙ„ Ù„Ø£Ù†Ù…Ø§Ø· Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS programming_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                pattern_name TEXT,
                pattern_description TEXT,
                code_example TEXT,
                use_cases TEXT,
                category TEXT,
                difficulty TEXT,
                source_url TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
        print("âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©")
    
    def crawl_tech_websites(self):
        """Ø²Ø­Ù Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªÙ‚Ù†ÙŠØ© Ø§Ù„Ø´Ù‡ÙŠØ±Ø©"""
        tech_sites = [
            {
                'name': 'Real Python',
                'url': 'https://realpython.com/tutorials/',
                'category': 'tutorials',
                'priority': 'high'
            },
            {
                'name': 'Python Official Docs',
                'url': 'https://docs.python.org/3/tutorial/',
                'category': 'documentation', 
                'priority': 'high'
            },
            {
                'name': 'GeeksforGeeks Python',
                'url': 'https://www.geeksforgeeks.org/python-programming-language/',
                'category': 'tutorials',
                'priority': 'medium'
            },
            {
                'name': 'W3Schools Python',
                'url': 'https://www.w3schools.com/python/',
                'category': 'tutorials',
                'priority': 'medium'
            }
        ]
        
        print("ğŸŒ Ø¨Ø¯Ø¡ Ø²Ø­Ù Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªÙ‚Ù†ÙŠØ©...")
        
        for site in tech_sites:
            print(f"ğŸ” ÙŠØ²Ø­Ù {site['name']}...")
            try:
                self.crawl_site(site['url'], site['category'])
                time.sleep(2)  # Ø§Ø­ØªØ±Ø§Ù… Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ù…ÙˆÙ‚Ø¹
            except Exception as e:
                print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø²Ø­Ù {site['name']}: {e}")
    
    def crawl_site(self, url, category):
        """Ø²Ø­Ù Ù…ÙˆÙ‚Ø¹ Ù…Ø¹ÙŠÙ†"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (compatible; HyperFactoryBot/1.0; +http://hyper-factory.com)'
            }
            response = requests.get(url, headers=headers, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
            title = soup.title.string if soup.title else "No Title"
            content = self.extract_meaningful_content(soup)
            
            if len(content) > 200:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù‚ØµÙŠØ±
                self.save_web_knowledge({
                    'title': title,
                    'content': content,
                    'url': url,
                    'source_type': 'website',
                    'category': category,
                    'difficulty': self.estimate_difficulty(content),
                    'tags': self.generate_tags(content, category),
                    'content_length': len(content),
                    'quality_score': self.calculate_quality_score(content)
                })
                
                print(f"âœ… ØªÙ… Ø­ÙØ¸: {title[:50]}...")
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù„Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø²Ø­Ù
            self.extract_and_crawl_links(soup, url, category)
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø²Ø­Ù {url}: {e}")
    
    def extract_meaningful_content(self, soup):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙÙŠØ¯ Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù†Ø§ØµØ± ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
        for element in soup(["script", "style", "nav", "footer", "header"]):
            element.decompose()
        
        content_parts = []
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
        for heading in soup.find_all(['h1', 'h2', 'h3']):
            text = heading.get_text().strip()
            if text and len(text) > 10:
                content_parts.append(f"## {text}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙÙ‚Ø±Ø§Øª
        for paragraph in soup.find_all('p'):
            text = paragraph.get_text().strip()
            if len(text) > 50:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø©
                content_parts.append(text)
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙˆØ¯ Ø§Ù„Ø¨Ø±Ù…Ø¬Ø©
        for code_block in soup.find_all(['pre', 'code']):
            code_text = code_block.get_text().strip()
            if len(code_text) > 20:
                content_parts.append(f"```python\n{code_text}\n```")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…
        for list_item in soup.find_all('li'):
            text = list_item.get_text().strip()
            if len(text) > 20:
                content_parts.append(f"â€¢ {text}")
        
        return '\n\n'.join(content_parts)
    
    def extract_and_crawl_links(self, soup, base_url, category):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙˆØ²Ø­Ù Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©"""
        try:
            links_found = 0
            for link in soup.find_all('a', href=True):
                href = link['href']
                full_url = urljoin(base_url, href)
                
                # ØªØµÙÙŠØ© Ø§Ù„Ø±ÙˆØ§Ø¨Ø·
                if self.should_crawl_link(full_url) and links_found < 5:
                    if full_url not in self.visited_urls:
                        self.visited_urls.add(full_url)
                        time.sleep(1)  # ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª
                        self.crawl_site(full_url, category)
                        links_found += 1
                        
        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø·: {e}")
    
    def should_crawl_link(self, url):
        """ØªØ­Ø¯ÙŠØ¯ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø²Ø­Ù Ø§Ù„Ø±Ø§Ø¨Ø·"""
        parsed = urlparse(url)
        
        # Ù‚ÙˆØ§Ø¦Ù… Ø§Ù„ØªØ¶Ù…ÙŠÙ† ÙˆØ§Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯
        include_keywords = ['python', 'tutorial', 'guide', 'example', 'how-to']
        exclude_keywords = ['login', 'signup', 'logout', 'admin', 'download']
        
        url_lower = url.lower()
        
        # Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        has_include = any(keyword in url_lower for keyword in include_keywords)
        has_exclude = any(keyword in url_lower for keyword in exclude_keywords)
        
        return has_include and not has_exclude and parsed.netloc
    
    def estimate_difficulty(self, content):
        """ØªÙ‚Ø¯ÙŠØ± ØµØ¹ÙˆØ¨Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        content_lower = content.lower()
        
        advanced_terms = ['asynchronous', 'decorator', 'generator', 'metaclass', 'multithreading']
        intermediate_terms = ['function', 'class', 'module', 'import', 'exception']
        
        advanced_count = sum(1 for term in advanced_terms if term in content_lower)
        intermediate_count = sum(1 for term in intermediate_terms if term in content_lower)
        
        if advanced_count > 2:
            return 'advanced'
        elif intermediate_count > 3:
            return 'intermediate'
        else:
            return 'beginner'
    
    def generate_tags(self, content, category):
        """Ø¥Ù†Ø´Ø§Ø¡ ÙˆØ³ÙˆÙ… Ù„Ù„Ù…Ø­ØªÙˆÙ‰"""
        content_lower = content.lower()
        tags = [category]
        
        # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ø´Ø§Ø¦Ø¹Ø©
        keywords = {
            'python': ['python', 'py'],
            'function': ['def ', 'function', 'lambda'],
            'class': ['class ', 'object', 'self'],
            'debug': ['debug', 'error', 'exception'],
            'web': ['flask', 'django', 'requests', 'api'],
            'data': ['pandas', 'numpy', 'dataframe', 'analysis']
        }
        
        for tag, terms in keywords.items():
            if any(term in content_lower for term in terms):
                tags.append(tag)
        
        return ','.join(tags)
    
    def calculate_quality_score(self, content):
        """Ø­Ø³Ø§Ø¨ Ø¬ÙˆØ¯Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        score = 0.0
        
        # Ø·ÙˆÙ„ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        if len(content) > 1000:
            score += 0.3
        elif len(content) > 500:
            score += 0.2
        elif len(content) > 200:
            score += 0.1
        
        # ÙˆØ¬ÙˆØ¯ ÙƒÙˆØ¯ Ø¨Ø±Ù…Ø¬ÙŠ
        if '```' in content or 'def ' in content or 'import ' in content:
            score += 0.3
        
        # ÙˆØ¬ÙˆØ¯ Ø£Ù…Ø«Ù„Ø©
        if 'example' in content.lower() or 'Ù…Ø«Ø§Ù„' in content:
            score += 0.2
        
        # ØªÙ†ÙˆØ¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰
        lines = content.split('\n')
        if len(lines) > 10:
            score += 0.2
        
        return min(score, 1.0)
    
    def save_web_knowledge(self, item):
        """Ø­ÙØ¸ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ø§Ù„ÙˆÙŠØ¨"""
        try:
            conn = sqlite3.connect(self.knowledge_db)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO web_knowledge 
                (title, content, url, source_type, category, difficulty, tags, content_length, quality_score)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                item['title'],
                item['content'],
                item['url'],
                item['source_type'],
                item['category'],
                item['difficulty'],
                item['tags'],
                item['content_length'],
                item['quality_score']
            ))
            
            conn.commit()
            conn.close()
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ Ø§Ù„Ù…Ø¹Ø±ÙØ©: {e}")
    
    def generate_knowledge_report(self):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø¹Ù† Ø§Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©"""
        conn = sqlite3.connect(self.knowledge_db)
        cursor = conn.cursor()
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©
        cursor.execute('SELECT COUNT(*) FROM web_knowledge')
        total_items = cursor.fetchone()[0]
        
        cursor.execute('SELECT COUNT(DISTINCT category) FROM web_knowledge')
        categories_count = cursor.fetchone()[0]
        
        cursor.execute('SELECT AVG(quality_score) FROM web_knowledge')
        avg_quality = cursor.fetchone()[0] or 0
        
        cursor.execute('''
            SELECT category, COUNT(*) as count 
            FROM web_knowledge 
            GROUP BY category 
            ORDER BY count DESC
        ''')
        categories_stats = cursor.fetchall()
        
        conn.close()
        
        report = {
            'report_date': datetime.now().isoformat(),
            'total_knowledge_items': total_items,
            'categories_count': categories_count,
            'average_quality_score': f"{avg_quality:.1%}",
            'categories_breakdown': dict(categories_stats),
            'visited_urls_count': len(self.visited_urls)
        }
        
        return report

def main():
    spider = AdvancedWebSpider()
    
    print("ğŸ•·ï¸ ØªØ´ØºÙŠÙ„ Advanced Web Spider")
    print("=" * 50)
    
    # Ø§Ù„Ø²Ø­Ù Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªÙ‚Ù†ÙŠØ©
    spider.crawl_tech_websites()
    
    # ØªÙˆÙ„ÙŠØ¯ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
    report = spider.generate_knowledge_report()
    
    print(f"\nğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø±ÙØ©:")
    print(f"   ğŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø¹Ù†Ø§ØµØ±: {report['total_knowledge_items']}")
    print(f"   ğŸ—‚ï¸ Ø¹Ø¯Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª: {report['categories_count']}")
    print(f"   â­ Ù…ØªÙˆØ³Ø· Ø§Ù„Ø¬ÙˆØ¯Ø©: {report['average_quality_score']}")
    print(f"   ğŸŒ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„Ù…Ø²Ø±ÙˆÙØ©: {report['visited_urls_count']}")
    
    print(f"\nğŸ“‹ ØªÙØµÙŠÙ„ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª:")
    for category, count in report['categories_breakdown'].items():
        print(f"   â€¢ {category}: {count} Ø¹Ù†ØµØ±")
    
    print(f"\nğŸ‰ Ø§ÙƒØªÙ…Ù„ Ø¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª!")

if __name__ == "__main__":
    main()
