#!/usr/bin/env python3
"""
Ù…Ø¯ÙŠØ± Ø§Ù„Ø²ÙˆØ§Ø­Ù - ÙŠØ¯ÙŠØ± Ø¬Ù…ÙŠØ¹ Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø²Ø­Ù Ø¨Ø´ÙƒÙ„ Ù…Ø±ÙƒØ²ÙŠ
"""

import sqlite3
import os
import json
from datetime import datetime

class CrawlerManager:
    def __init__(self):
        self.knowledge_db = "data/knowledge/knowledge.db"
        self.crawler_config = "config/crawler_config.json"
        self.setup_environment()
    
    def setup_environment(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ø¨ÙŠØ¦Ø©"""
        os.makedirs("config", exist_ok=True)
        os.makedirs("reports/management", exist_ok=True)
        
        if not os.path.exists(self.crawler_config):
            config = {
                "max_depth": 3,
                "max_pages_per_session": 100,
                "delay_between_requests": 1,
                "allowed_domains": [
                    "docs.python.org",
                    "realpython.com", 
                    "www.w3schools.com",
                    "stackoverflow.com"
                ],
                "banned_domains": [],
                "auto_cleanup": True,
                "learning_enabled": True
            }
            
            with open(self.crawler_config, 'w') as f:
                json.dump(config, f, indent=2)
    
    def get_system_stats(self):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù†Ø¸Ø§Ù…"""
        stats = {}
        
        try:
            conn = sqlite3.connect(self.knowledge_db)
            cursor = conn.cursor()
            
            # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø¹Ø§Ù…Ø©
            cursor.execute('SELECT COUNT(*) FROM web_knowledge')
            stats['total_pages'] = cursor.fetchone()[0]
            
            cursor.execute('SELECT COUNT(DISTINCT url) FROM web_knowledge')
            stats['unique_urls'] = cursor.fetchone()[0]
            
            cursor.execute('SELECT COUNT(DISTINCT category) FROM web_knowledge')
            stats['categories_count'] = cursor.fetchone()[0]
            
            # Ø£Ø­Ø¯Ø« Ø§Ù„Ø¥Ø¶Ø§ÙØ§Øª
            cursor.execute('''
                SELECT url, title, created_at 
                FROM web_knowledge 
                ORDER BY created_at DESC 
                LIMIT 5
            ''')
            stats['recent_additions'] = cursor.fetchall()
            
            # ØªÙˆØ²ÙŠØ¹ Ø§Ù„ÙØ¦Ø§Øª
            cursor.execute('''
                SELECT category, COUNT(*) 
                FROM web_knowledge 
                GROUP BY category 
                ORDER BY COUNT(*) DESC
            ''')
            stats['category_distribution'] = cursor.fetchall()
            
            conn.close()
            
        except Exception as e:
            stats['error'] = str(e)
        
        return stats
    
    def generate_management_report(self):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø¥Ø¯Ø§Ø±ÙŠ"""
        print("ğŸ“Š ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø²ÙˆØ§Ø­Ù...")
        
        stats = self.get_system_stats()
        
        report = {
            "report_date": datetime.now().isoformat(),
            "system_stats": stats,
            "recommendations": [],
            "maintenance_tasks": []
        }
        
        # ØªØ­Ù„ÙŠÙ„ ÙˆØªÙˆØµÙŠØ§Øª
        if 'total_pages' in stats and stats['total_pages'] > 500:
            report["recommendations"].append("ğŸ’¡ ÙÙƒØ± ÙÙŠ ØªÙ†Ø¸ÙŠÙ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù‚Ø¯ÙŠÙ…")
        
        if 'categories_count' in stats and stats['categories_count'] < 3:
            report["recommendations"].append("ğŸ’¡ Ø¬Ø±Ø¨ Ø²Ø­Ù Ù…ÙˆØ§Ù‚Ø¹ Ù…Ù† ÙØ¦Ø§Øª Ù…ØªÙ†ÙˆØ¹Ø©")
        
        # Ù…Ù‡Ø§Ù… Ø§Ù„ØµÙŠØ§Ù†Ø©
        report["maintenance_tasks"] = [
            "ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ù…ÙƒØ±Ø±Ø©",
            "ØªØ­Ø³ÙŠÙ† ÙÙ‡Ø§Ø±Ø³ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª", 
            "ØªØ­Ø¯ÙŠØ« Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø³Ù…ÙˆØ­Ø©",
            "Ù…Ø±Ø§Ø¬Ø¹Ø© ØªÙ‚Ø§Ø±ÙŠØ± Ø§Ù„Ø£Ø¯Ø§Ø¡"
        ]
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        report_file = f"reports/management/crawler_management_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        # Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {report_file}")
        print(f"ğŸ“ˆ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØµÙØ­Ø§Øª: {stats.get('total_pages', 'N/A')}")
        print(f"ğŸŒ Ø±ÙˆØ§Ø¨Ø· ÙØ±ÙŠØ¯Ø©: {stats.get('unique_urls', 'N/A')}")
        print(f"ğŸ·ï¸  Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª: {stats.get('categories_count', 'N/A')}")
        
        if 'recent_additions' in stats:
            print("\nğŸ“¥ Ø£Ø­Ø¯Ø« Ø§Ù„Ø¥Ø¶Ø§ÙØ§Øª:")
            for url, title, date in stats['recent_additions']:
                print(f"   - {title[:40]}...")
        
        if report["recommendations"]:
            print("\nğŸ’¡ Ø§Ù„ØªÙˆØµÙŠØ§Øª:")
            for rec in report["recommendations"]:
                print(f"   {rec}")
    
    def run_maintenance(self):
        """ØªØ´ØºÙŠÙ„ Ù…Ù‡Ø§Ù… Ø§Ù„ØµÙŠØ§Ù†Ø©"""
        print("ğŸ”§ ØªØ´ØºÙŠÙ„ Ù…Ù‡Ø§Ù… ØµÙŠØ§Ù†Ø© Ø§Ù„Ø²ÙˆØ§Ø­Ù...")
        
        # Ø§Ø³ØªÙŠØ±Ø§Ø¯ ÙˆØ¥Ø¬Ø±Ø§Ø¡ Ø§Ù„ØµÙŠØ§Ù†Ø©
        import subprocess
        import sys
        
        try:
            # ØªØ´ØºÙŠÙ„ Ø¥ØµÙ„Ø§Ø­ Ø§Ù„Ø²Ø­Ù
            result = subprocess.run([
                sys.executable, "scripts/fix_crawler_issues.py"
            ], capture_output=True, text=True, cwd=os.getcwd())
            
            print("âœ… Ø§ÙƒØªÙ…Ù„Øª Ù…Ù‡Ø§Ù… Ø§Ù„ØµÙŠØ§Ù†Ø©")
            print(result.stdout)
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØµÙŠØ§Ù†Ø©: {e}")
    
    def show_dashboard(self):
        """Ø¹Ø±Ø¶ Ù„ÙˆØ­Ø© ØªØ­ÙƒÙ… Ø§Ù„Ø²ÙˆØ§Ø­Ù"""
        stats = self.get_system_stats()
        
        print("""
ğŸš€ Ù„ÙˆØ­Ø© ØªØ­ÙƒÙ… Ù…Ø¯ÙŠØ± Ø§Ù„Ø²ÙˆØ§Ø­Ù
==========================

ğŸ“Š Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª:
   ğŸ“ Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø®Ø²Ù†Ø©: {pages}
   ğŸŒ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„ÙØ±ÙŠØ¯Ø©: {urls}
   ğŸ·ï¸  Ø§Ù„ÙØ¦Ø§Øª Ø§Ù„Ù†Ø´Ø·Ø©: {categories}

âš¡ Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø§Ù„Ø³Ø±ÙŠØ¹Ø©:
   1. Ø¹Ø±Ø¶ ØªÙ‚Ø±ÙŠØ± Ù…ÙØµÙ„
   2. ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙŠØ§Ù†Ø©
   3. ØªØ´ØºÙŠÙ„ Ø²Ø§Ø­Ù Ø¢Ù…Ù†
   4. ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ù†Ø¸Ø§Ù…
   5. Ø§Ù„Ø®Ø±ÙˆØ¬

        """.format(
            pages=stats.get('total_pages', 'N/A'),
            urls=stats.get('unique_urls', 'N/A'), 
            categories=stats.get('categories_count', 'N/A')
        ))

if __name__ == "__main__":
    manager = CrawlerManager()
    manager.show_dashboard()
    manager.generate_management_report()
    
    # Ø¹Ø±Ø¶ Ø®ÙŠØ§Ø± Ù„Ù„Ù…Ø³ØªØ®Ø¯Ù…
    choice = input("\nØ§Ø®ØªØ± Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡ (1-5): ").strip()
    
    if choice == "1":
        manager.generate_management_report()
    elif choice == "2":
        manager.run_maintenance()
    elif choice == "3":
        print("ğŸš€ ØªØ´ØºÙŠÙ„ Ø§Ù„Ø²Ø§Ø­Ù Ø§Ù„Ø¢Ù…Ù†...")
        import subprocess
        subprocess.run(["python3", "tools/hf_web_spider_optimized.py"])
    elif choice == "4":
        print("ğŸ” ÙØ­Øµ ØµØ­Ø© Ø§Ù„Ù†Ø¸Ø§Ù…...")
        import subprocess
        subprocess.run(["python3", "scripts/fix_crawler_issues.py"])
    else:
        print("ğŸ‘‹ Ù…Ø¹ Ø§Ù„Ø³Ù„Ø§Ù…Ø©!")
