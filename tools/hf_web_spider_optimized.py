#!/usr/bin/env python3
"""
Web Spider Ø§Ù„Ù…Ø­Ø³Ù† - Ù…Ø¹ Ø­Ø¯ÙˆØ¯ Ù„Ù„ØªØ¹Ù…Ù‚ ÙˆØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
"""

import requests
import sqlite3
import json
import os
import time
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
from datetime import datetime
import sys

class OptimizedWebSpider:
    def __init__(self, max_depth=3, max_pages=100, delay=1):
        self.knowledge_db = "data/knowledge/knowledge.db"
        self.visited_urls = set()
        self.urls_to_visit = []
        self.max_depth = max_depth
        self.max_pages = max_pages
        self.delay = delay
        self.crawled_count = 0
        self.setup_database()
        
        # Ø²ÙŠØ§Ø¯Ø© Ø¹Ù…Ù‚ Ø§Ù„Ø¹ÙˆØ¯ÙŠØ© Ù„Ù„Ù†Ø¸Ø§Ù…
        sys.setrecursionlimit(10000)
    
    def setup_database(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø´ÙƒÙ„ Ø¢Ù…Ù†"""
        os.makedirs("data/knowledge", exist_ok=True)
        conn = sqlite3.connect(self.knowledge_db)
        cursor = conn.cursor()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„ Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…ÙˆØ¬ÙˆØ¯Ø©
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS web_knowledge (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                url TEXT UNIQUE,
                title TEXT,
                content TEXT,
                summary TEXT,
                category TEXT,
                tags TEXT,
                importance INTEGER DEFAULT 1,
                depth INTEGER DEFAULT 0,
                last_updated DATETIME DEFAULT CURRENT_TIMESTAMP,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        
        conn.commit()
        conn.close()
        print("âœ… ØªÙ… Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª")
    
    def is_valid_url(self, url):
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† ØµØ­Ø© Ø§Ù„Ø±Ø§Ø¨Ø·"""
        try:
            parsed = urlparse(url)
            return bool(parsed.netloc) and bool(parsed.scheme)
        except Exception:
            return False
    
    def should_crawl(self, url, depth):
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø²Ø­Ù Ø§Ù„Ø±Ø§Ø¨Ø·"""
        if depth > self.max_depth:
            return False
        
        if self.crawled_count >= self.max_pages:
            return False
        
        if url in self.visited_urls:
            return False
        
        # ØªØ¬Ù†Ø¨ Ø£Ù†ÙˆØ§Ø¹ Ø§Ù„Ù…Ù„ÙØ§Øª ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
        excluded_extensions = ['.pdf', '.doc', '.docx', '.zip', '.tar', '.gz']
        if any(url.lower().endswith(ext) for ext in excluded_extensions):
            return False
        
        return True
    
    def extract_content(self, soup):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
        # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„Ø¹Ù„Ø§Ù…Ø§Øª ØºÙŠØ± Ø§Ù„Ù…Ø±ØºÙˆØ¨Ø©
        for script in soup(["script", "style", "nav", "footer", "header"]):
            script.decompose()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ
        text = soup.get_text()
        
        # ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù†Øµ
        lines = (line.strip() for line in text.splitlines())
        chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
        text = ' '.join(chunk for chunk in chunks if chunk)
        
        return text
    
    def generate_summary(self, content, max_length=200):
        """ØªÙˆÙ„ÙŠØ¯ Ù…Ù„Ø®Øµ Ù„Ù„Ù…Ø­ØªÙˆÙ‰"""
        if len(content) <= max_length:
            return content
        
        # Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ù†Ù‚Ø·Ø© ØªÙˆÙ‚Ù Ù…Ù†Ø·Ù‚ÙŠØ©
        sentences = re.split(r'[.!?]+', content)
        summary = ""
        
        for sentence in sentences:
            if len(summary + sentence) < max_length:
                summary += sentence + ". "
            else:
                break
        
        return summary.strip() or content[:max_length] + "..."
    
    def categorize_content(self, title, content, url):
        """ØªØµÙ†ÙŠÙ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹"""
        categories = {
            'python': r'python|numpy|pandas|django|flask',
            'programming': r'programming|code|algorithm|software|developer',
            'tutorial': r'tutorial|guide|how to|example|step by step',
            'documentation': r'documentation|api|reference|manual',
            'discussion': r'discussion|forum|mailing list|thread',
            'news': r'news|release|update|announcement'
        }
        
        text = (title + " " + content).lower()
        
        for category, pattern in categories.items():
            if re.search(pattern, text, re.IGNORECASE):
                return category
        
        return 'general'
    
    def extract_tags(self, title, content):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙˆØ³ÙˆÙ… Ù…Ù† Ø§Ù„Ù…Ø­ØªÙˆÙ‰"""
        words = re.findall(r'\b[a-zA-Z]{4,}\b', title + " " + content)
        common_words = {'this', 'that', 'with', 'from', 'have', 'been', 'will', 'your', 'more', 'when'}
        
        tags = [word.lower() for word in words[:10] 
                if word.lower() not in common_words and len(word) > 3]
        
        return list(set(tags))[:5]
    
    def save_to_database(self, url, title, content, depth):
        """Ø­ÙØ¸ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©"""
        try:
            summary = self.generate_summary(content)
            category = self.categorize_content(title, content, url)
            tags = ",".join(self.extract_tags(title, content))
            
            conn = sqlite3.connect(self.knowledge_db)
            cursor = conn.cursor()
            
            cursor.execute('''
                INSERT OR REPLACE INTO web_knowledge 
                (url, title, content, summary, category, tags, depth)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            ''', (url, title, content, summary, category, tags, depth))
            
            conn.commit()
            conn.close()
            
            return True
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø­ÙØ¸ {url}: {e}")
            return False
    
    def crawl_page(self, url, depth=0):
        """Ø²Ø­Ù ØµÙØ­Ø© ÙØ±Ø¯ÙŠØ©"""
        if not self.should_crawl(url, depth):
            return []
        
        print(f"ğŸ” ÙŠØ²Ø­Ù [{depth}]: {url}")
        
        try:
            # ØªØ£Ø®ÙŠØ± Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª
            time.sleep(self.delay)
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¹Ù†ÙˆØ§Ù† ÙˆØ§Ù„Ù…Ø­ØªÙˆÙ‰
            title = soup.title.string if soup.title else "No Title"
            content = self.extract_content(soup)
            
            # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            if self.save_to_database(url, title, content, depth):
                print(f"âœ… ØªÙ… Ø­ÙØ¸: {title[:50]}...")
                self.crawled_count += 1
            
            self.visited_urls.add(url)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ù„Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ§Ù„ÙŠ
            if depth < self.max_depth:
                links = []
                for link in soup.find_all('a', href=True):
                    href = link['href']
                    full_url = urljoin(url, href)
                    
                    if self.is_valid_url(full_url) and full_url not in self.visited_urls:
                        links.append(full_url)
                
                return links
            
            return []
            
        except requests.RequestException as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø²Ø­Ù {url}: {e}")
            return []
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹ ÙÙŠ {url}: {e}")
            return []
    
    def crawl_site(self, start_urls):
        """Ø¨Ø¯Ø¡ Ø§Ù„Ø²Ø­Ù Ù…Ù† Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©"""
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø²Ø­Ù Ø§Ù„Ù…Ø­Ø³Ù†...")
        print(f"ğŸ“Š Ø§Ù„Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª: Ø¹Ù…Ù‚ {self.max_depth}, Ø­Ø¯ {self.max_pages} ØµÙØ­Ø©")
        
        self.urls_to_visit = [(url, 0) for url in start_urls]
        
        while self.urls_to_visit and self.crawled_count < self.max_pages:
            url, depth = self.urls_to_visit.pop(0)
            
            if url not in self.visited_urls:
                new_links = self.crawl_page(url, depth)
                
                # Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù„Ù„Ù…Ø³ØªÙˆÙ‰ Ø§Ù„ØªØ§Ù„ÙŠ
                if depth < self.max_depth:
                    for link in new_links:
                        if link not in [u for u, d in self.urls_to_visit]:
                            self.urls_to_visit.append((link, depth + 1))
        
        self.generate_report()
    
    def generate_report(self):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø¹Ù† Ø§Ù„Ø²Ø­Ù"""
        try:
            conn = sqlite3.connect(self.knowledge_db)
            cursor = conn.cursor()
            
            cursor.execute('SELECT COUNT(*) FROM web_knowledge')
            total_records = cursor.fetchone()[0]
            
            cursor.execute('SELECT COUNT(DISTINCT category) FROM web_knowledge')
            categories_count = cursor.fetchone()[0]
            
            cursor.execute('''
                SELECT category, COUNT(*) 
                FROM web_knowledge 
                GROUP BY category 
                ORDER BY COUNT(*) DESC
            ''')
            categories = cursor.fetchall()
            
            conn.close()
            
            report = {
                "timestamp": datetime.now().isoformat(),
                "total_crawled": self.crawled_count,
                "total_in_database": total_records,
                "unique_categories": categories_count,
                "categories_breakdown": dict(categories),
                "max_depth": self.max_depth,
                "max_pages": self.max_pages
            }
            
            # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
            os.makedirs("reports/diagnostics", exist_ok=True)
            report_file = f"reports/diagnostics/web_spider_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            with open(report_file, 'w') as f:
                json.dump(report, f, indent=2)
            
            print("\nğŸ“Š ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ø²Ø­Ù:")
            print(f"   âœ… Ø§Ù„ØµÙØ­Ø§Øª Ø§Ù„Ù…Ø²Ø­ÙˆÙØ©: {self.crawled_count}")
            print(f"   ğŸ“ Ø§Ù„Ø³Ø¬Ù„Ø§Øª ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {total_records}")
            print(f"   ğŸ·ï¸  Ø§Ù„ÙØ¦Ø§Øª: {categories_count}")
            print(f"   ğŸ“„ Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„ÙƒØ§Ù…Ù„: {report_file}")
            
        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø£ ÙÙŠ Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {e}")
    
    def safe_crawl(self, start_urls):
        """Ø²Ø­Ù Ø¢Ù…Ù† Ù…Ø¹ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø£Ø®Ø·Ø§Ø¡"""
        try:
            self.crawl_site(start_urls)
            print("ğŸ‰ Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø²Ø­Ù Ø§Ù„Ø¢Ù…Ù†!")
        except KeyboardInterrupt:
            print("\nâ¹ï¸  ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„Ø²Ø­Ù Ø¨ÙˆØ§Ø³Ø·Ø© Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…")
            self.generate_report()
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ Ø¬Ø³ÙŠÙ… ÙÙŠ Ø§Ù„Ø²Ø­Ù: {e}")
            self.generate_report()

# Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù…
if __name__ == "__main__":
    # Ø±ÙˆØ§Ø¨Ø· Ø¨Ø¯Ø§ÙŠØ© Ø¢Ù…Ù†Ø© ÙˆÙ…Ø­Ø¯ÙˆØ¯Ø©
    safe_start_urls = [
        "https://docs.python.org/3/tutorial/",
        "https://realpython.com/python-basics/",
        "https://www.w3schools.com/python/"
    ]
    
    spider = OptimizedWebSpider(
        max_depth=2,      # Ø¹Ù…Ù‚ Ù…Ø­Ø¯ÙˆØ¯
        max_pages=50,     # Ø¹Ø¯Ø¯ ØµÙØ­Ø§Øª Ù…Ø­Ø¯ÙˆØ¯
        delay=1          # ØªØ£Ø®ÙŠØ± 1 Ø«Ø§Ù†ÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª
    )
    
    spider.safe_crawl(safe_start_urls)
