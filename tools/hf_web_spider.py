#!/usr/bin/env python3
"""
Ø²Ø§Ø­Ù Ø§Ù„ÙˆÙŠØ¨ Ø§Ù„Ù…ØªÙ‚Ø¯Ù… - ÙŠØ¬Ù…Ø¹ Ø§Ù„Ù…Ø¹Ø±ÙØ© Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª
"""

import requests
import sqlite3
import json
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import time
import os
from datetime import datetime

class AdvancedWebSpider:
    def __init__(self):
        self.knowledge_db = "data/knowledge/knowledge.db"
        self.setup_database()
        
    def setup_database(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        os.makedirs("data/knowledge", exist_ok=True)
        conn = sqlite3.connect(self.knowledge_db)
        cursor = conn.cursor()
        
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS web_knowledge (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                title TEXT,
                content TEXT,
                url TEXT,
                source_type TEXT,
                category TEXT,
                difficulty TEXT,
                tags TEXT,
                crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            )
        ''')
        conn.commit()
        conn.close()
        
    def crawl_website(self, url, max_pages=10):
        """Ø²Ø­Ù Ù…ÙˆÙ‚Ø¹ ÙˆÙŠØ¨"""
        print(f"ğŸ” ÙŠØ²Ø­Ù {url}...")
        
        try:
            response = requests.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª
            title = soup.title.string if soup.title else "No Title"
            content = self.extract_meaningful_content(soup)
            
            # Ø­ÙØ¸ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
            self.save_knowledge_item({
                'title': title,
                'content': content,
                'url': url,
                'source_type': 'website',
                'category': 'programming',
                'difficulty': 'intermediate',
                'tags': 'web,crawled,programming'
            })
            
            print(f"âœ… ØªÙ… Ø²Ø­Ù: {title}")
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ Ø²Ø­Ù {url}: {e}")
    
    def extract_meaningful_content(self, soup):
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…ÙÙŠØ¯ Ù…Ù† Ø§Ù„ØµÙØ­Ø©"""
        # Ø¥Ø²Ø§Ù„Ø© scripts Ùˆstyles
        for script in soup(["script", "style"]):
            script.decompose()
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù†Øµ Ù…Ù† Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ† ÙˆØ§Ù„ÙÙ‚Ø±Ø§Øª
        content_parts = []
        
        # Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†
        for heading in soup.find_all(['h1', 'h2', 'h3']):
            content_parts.append(f"## {heading.get_text().strip()}")
        
        # Ø§Ù„ÙÙ‚Ø±Ø§Øª
        for paragraph in soup.find_all('p'):
            text = paragraph.get_text().strip()
            if len(text) > 50:  # ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù‚ØµÙŠØ±Ø©
                content_parts.append(text)
        
        # Ø§Ù„Ù‚ÙˆØ§Ø¦Ù…
        for list_item in soup.find_all('li'):
            text = list_item.get_text().strip()
            if len(text) > 20:
                content_parts.append(f"- {text}")
        
        return '\n'.join(content_parts)
    
    def save_knowledge_item(self, item):
        """Ø­ÙØ¸ Ø¹Ù†ØµØ± Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        conn = sqlite3.connect(self.knowledge_db)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT INTO web_knowledge 
            (title, content, url, source_type, category, difficulty, tags)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            item['title'],
            item['content'],
            item['url'],
            item['source_type'],
            item['category'],
            item['difficulty'],
            item['tags']
        ))
        
        conn.commit()
        conn.close()
    
    def search_and_crawl(self, query, max_results=5):
        """Ø§Ù„Ø¨search ÙˆØ§Ù„Ø²Ø­Ù Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ query"""
        print(f"ğŸ” ÙŠØ¨Ø­Ø« Ø¹Ù†: {query}")
        
        # Ù…Ø­Ø§ÙƒØ§Ø© Ø§Ù„Ø¨Ø­Ø« (ÙÙŠ Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ù†Ø³ØªØ®Ø¯Ù… Google Custom Search API)
        search_urls = self.generate_search_urls(query, max_results)
        
        for url in search_urls:
            self.crawl_website(url)
            time.sleep(2)  # Ø§Ø­ØªØ±Ø§Ù… Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ù…ÙˆÙ‚Ø¹
    
    def generate_search_urls(self, query, max_results):
        """Ø¥Ù†Ø´Ø§Ø¡ Ø±ÙˆØ§Ø¨Ø· Ø¨Ø­Ø« (Ù…Ø­Ø§ÙƒØ§Ø©)"""
        # ÙÙŠ Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØŒ Ù†Ø³ØªØ®Ø¯Ù… API Ø§Ù„Ø¨Ø­Ø«
        base_urls = [
            f"https://github.com/search?q={query.replace(' ', '+')}",
            f"https://stackoverflow.com/search?q={query.replace(' ', '+')}",
            f"https://realpython.com/search?q={query.replace(' ', '+')}",
            f"https://docs.python.org/3/search.html?q={query.replace(' ', '+')}"
        ]
        return base_urls[:max_results]
    
    def run_auto_crawl(self):
        """ØªØ´ØºÙŠÙ„ Ø§Ù„Ø²Ø­Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ"""
        print("ğŸš€ Ø¨Ø¯Ø¡ Ø§Ù„Ø²Ø­Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª...")
        
        # ÙƒÙ„Ù…Ø§Øª Ù…ÙØªØ§Ø­ÙŠØ© Ù„Ù„Ø¨Ø­Ø«
        keywords = [
            "Python debugging techniques",
            "AI agents architecture", 
            "system design patterns",
            "machine learning basics",
            "web scraping Python"
        ]
        
        for keyword in keywords:
            print(f"\nğŸ“– ÙŠØ¨Ø­Ø« Ø¹Ù†: {keyword}")
            self.search_and_crawl(keyword, max_results=3)
            time.sleep(3)

def main():
    spider = AdvancedWebSpider()
    
    print("ğŸ•·ï¸ ØªØ´ØºÙŠÙ„ Advanced Web Spider")
    print("=" * 40)
    
    # Ø§Ù„Ø²Ø­Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ
    spider.run_auto_crawl()
    
    # Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
    conn = sqlite3.connect(spider.knowledge_db)
    cursor = conn.cursor()
    cursor.execute("SELECT COUNT(*) FROM web_knowledge")
    count = cursor.fetchone()[0]
    conn.close()
    
    print(f"\nğŸ‰ Ø§ÙƒØªÙ…Ù„ Ø§Ù„Ø²Ø­Ù! ØªÙ… Ø¬Ù…Ø¹ {count} Ø¹Ù†ØµØ± Ù…Ø¹Ø±ÙØ© Ù…Ù† Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª")

if __name__ == "__main__":
    main()
