#!/usr/bin/env python3
"""
Ø§Ù„Ø²Ø§Ø­Ù Ø§Ù„Ø°ÙƒÙŠ - ÙŠØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙˆÙŠØªÙƒÙŠÙ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
"""

import sqlite3
import requests
import time
import random
from datetime import datetime, timedelta
from urllib.parse import urlparse
import re
import os
import json

class SmartCrawler:
    def __init__(self):
        self.knowledge_db = "data/knowledge/knowledge.db"
        self.learning_file = "ai/memory/crawler_learning.json"
        self.setup_learning()
    
    def setup_learning(self):
        """Ø¥Ø¹Ø¯Ø§Ø¯ Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ¹Ù„Ù…"""
        os.makedirs("ai/memory", exist_ok=True)
        
        if not os.path.exists(self.learning_file):
            learning_data = {
                "successful_domains": {},
                "failed_domains": {},
                "optimal_delays": {},
                "learned_patterns": [],
                "last_analysis": datetime.now().isoformat()
            }
            
            with open(self.learning_file, 'w') as f:
                json.dump(learning_data, f, indent=2)
    
    def learn_from_experience(self, url, success, response_time=None):
        """Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† ØªØ¬Ø§Ø±Ø¨ Ø§Ù„Ø²Ø­Ù"""
        try:
            with open(self.learning_file, 'r') as f:
                learning_data = json.load(f)
            
            domain = urlparse(url).netloc
            
            if success:
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ù†Ø¬Ø§Ø­Ø§Øª
                if domain in learning_data["successful_domains"]:
                    learning_data["successful_domains"][domain] += 1
                else:
                    learning_data["successful_domains"][domain] = 1
                
                # ØªØ­Ø¯ÙŠØ« Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ø§Ø³ØªØ¬Ø§Ø¨Ø© Ø§Ù„Ù…Ø«Ù„Ù‰
                if response_time:
                    if domain in learning_data["optimal_delays"]:
                        current_delay = learning_data["optimal_delays"][domain]
                        # Ù…ØªÙˆØ³Ø· Ù…ØªØ­Ø±Ùƒ
                        new_delay = (current_delay + response_time) / 2
                        learning_data["optimal_delays"][domain] = min(new_delay, 5.0)
                    else:
                        learning_data["optimal_delays"][domain] = response_time
            else:
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„ÙØ´Ù„
                if domain in learning_data["failed_domains"]:
                    learning_data["failed_domains"][domain] += 1
                else:
                    learning_data["failed_domains"][domain] = 1
            
            learning_data["last_analysis"] = datetime.now().isoformat()
            
            with open(self.learning_file, 'w') as f:
                json.dump(learning_data, f, indent=2)
                
        except Exception as e:
            print(f"âš ï¸  Ø®Ø·Ø£ ÙÙŠ Ø§Ù„ØªØ¹Ù„Ù…: {e}")
    
    def get_optimal_delay(self, url):
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ØªØ£Ø®ÙŠØ± Ø§Ù„Ø£Ù…Ø«Ù„ Ù„Ù„Ù…Ø¬Ø§Ù„"""
        try:
            with open(self.learning_file, 'r') as f:
                learning_data = json.load(f)
            
            domain = urlparse(url).netloc
            
            if domain in learning_data["optimal_delays"]:
                return learning_data["optimal_delays"][domain]
            else:
                # ØªØ£Ø®ÙŠØ± Ø§ÙØªØ±Ø§Ø¶ÙŠ Ù…Ø¹ Ø¨Ø¹Ø¶ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ©
                return 1.0 + random.uniform(0, 1.0)
                
        except:
            return 1.5  # ØªØ£Ø®ÙŠØ± Ø§ÙØªØ±Ø§Ø¶ÙŠ Ø¢Ù…Ù†
    
    def should_crawl_domain(self, url):
        """ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† ÙŠØ¬Ø¨ Ø²Ø­Ù Ø§Ù„Ù…Ø¬Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø¨Ø±Ø©"""
        try:
            with open(self.learning_file, 'r') as f:
                learning_data = json.load(f)
            
            domain = urlparse(url).netloc
            
            # Ø¥Ø°Ø§ ÙØ´Ù„ Ø£ÙƒØ«Ø± Ù…Ù† 5 Ù…Ø±Ø§ØªØŒ ØªØ¬Ù†Ø¨Ù‡
            fail_count = learning_data["failed_domains"].get(domain, 0)
            if fail_count > 5:
                return False
            
            # Ø¥Ø°Ø§ Ù†Ø¬Ø­ ÙƒØ«ÙŠØ±Ø§Ù‹ØŒ Ø²Ø­ÙÙ‡ Ø£ÙˆÙ„Ø§Ù‹
            success_count = learning_data["successful_domains"].get(domain, 0)
            if success_count > 3:
                return True
            
            # Ù…Ø¬Ø§Ù„Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© - Ø¬Ø±Ø¨Ù‡Ø§
            return True
            
        except:
            return True
    
    def smart_crawl(self, url):
        """Ø²Ø­Ù Ø°ÙƒÙŠ Ù…Ø¹ Ø§Ù„ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„ØªØ¬Ø§Ø±Ø¨"""
        if not self.should_crawl_domain(url):
            print(f"â­ï¸  ØªØ®Ø·ÙŠ Ø§Ù„Ù…Ø¬Ø§Ù„ Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„Ø®Ø¨Ø±Ø©: {url}")
            return None
        
        optimal_delay = self.get_optimal_delay(url)
        time.sleep(optimal_delay)
        
        start_time = time.time()
        
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            response_time = time.time() - start_time
            
            # ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„Ù†Ø¬Ø§Ø­
            self.learn_from_experience(url, True, response_time)
            
            return response
            
        except Exception as e:
            response_time = time.time() - start_time
            
            # ØªØ¹Ù„Ù… Ù…Ù† Ø§Ù„ÙØ´Ù„
            self.learn_from_experience(url, False, response_time)
            
            print(f"âŒ ÙØ´Ù„ Ø°ÙƒÙŠ ÙÙŠ {url}: {e}")
            return None
    
    def generate_learning_report(self):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ¹Ù„Ù…"""
        try:
            with open(self.learning_file, 'r') as f:
                learning_data = json.load(f)
            
            print("ğŸ§  ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ù„Ù„Ø²Ø§Ø­Ù:")
            print(f"   ğŸ“Š Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„Ù†Ø§Ø¬Ø­Ø©: {len(learning_data['successful_domains'])}")
            print(f"   âš ï¸  Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø§Ù„ÙØ§Ø´Ù„Ø©: {len(learning_data['failed_domains'])}")
            print(f"   â±ï¸  Ø§Ù„ØªØ£Ø®ÙŠØ±Ø§Øª Ø§Ù„Ù…Ø«Ù„Ù‰: {len(learning_data['optimal_delays'])}")
            
            # Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø£Ø¯Ø§Ø¡Ù‹
            successful_domains = sorted(
                learning_data["successful_domains"].items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]
            
            if successful_domains:
                print("   ğŸ† Ø£ÙØ¶Ù„ Ø§Ù„Ù…Ø¬Ø§Ù„Ø§Øª Ø£Ø¯Ø§Ø¡Ù‹:")
                for domain, count in successful_domains:
                    optimal_delay = learning_data["optimal_delays"].get(domain, "N/A")
                    if optimal_delay != "N/A":
                        print(f"      - {domain}: {count} Ù†Ø¬Ø§Ø­, ØªØ£Ø®ÙŠØ± {optimal_delay:.2f}s")
                    else:
                        print(f"      - {domain}: {count} Ù†Ø¬Ø§Ø­")
            
        except Exception as e:
            print(f"âŒ Ø®Ø·Ø£ ÙÙŠ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØªØ¹Ù„Ù…: {e}")

if __name__ == "__main__":
    crawler = SmartCrawler()
    
    # Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø²Ø§Ø­Ù Ø§Ù„Ø°ÙƒÙŠ
    test_urls = [
        "https://docs.python.org/3/",
        "https://realpython.com/",
        "https://www.w3schools.com/python/"
    ]
    
    for url in test_urls:
        print(f"ğŸ” Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ø²Ø§Ø­Ù Ø§Ù„Ø°ÙƒÙŠ: {url}")
        response = crawler.smart_crawl(url)
        
        if response:
            print(f"âœ… Ù†Ø¬Ø§Ø­: {len(response.content)} bytes")
        else:
            print("âŒ ÙØ´Ù„")
        
        print()
    
    crawler.generate_learning_report()
