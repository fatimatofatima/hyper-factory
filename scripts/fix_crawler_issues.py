#!/usr/bin/env python3
"""
Ø¥ØµÙ„Ø§Ø­ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø²Ø­Ù - Ø§Ù„Ø¥ØµØ¯Ø§Ø± Ø§Ù„Ù…ØµØ­Ø­ Ù„Ù„Ù…Ø³Ø§Ø±Ø§Øª
"""

import sqlite3
import os
import requests
import json
from datetime import datetime

class CrawlerFix:
    def __init__(self):
        self.knowledge_db = "data/knowledge/knowledge.db"
    
    def analyze_crawler_issues(self):
        """ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø²Ø­Ù Ø§Ù„Ø­Ø§Ù„ÙŠØ©"""
        print("ğŸ” ØªØ­Ù„ÙŠÙ„ Ù…Ø´Ø§ÙƒÙ„ Ø§Ù„Ø²Ø­Ù...")
        
        issues = []
        
        # ÙØ­Øµ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        try:
            conn = sqlite3.connect(self.knowledge_db)
            cursor = conn.cursor()
            
            # ÙØ­Øµ Ø¬Ø¯ÙˆÙ„ web_knowledge
            cursor.execute('SELECT COUNT(*) FROM web_knowledge')
            total_records = cursor.fetchone()[0]
            
            cursor.execute('SELECT COUNT(DISTINCT url) FROM web_knowledge')
            unique_urls = cursor.fetchone()[0]
            
            cursor.execute('SELECT COUNT(DISTINCT category) FROM web_knowledge')
            categories_count = cursor.fetchone()[0]
            
            conn.close()
            
            print(f"ğŸ“Š Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ©:")
            print(f"   ğŸ“ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª: {total_records}")
            print(f"   ğŸŒ Ø±ÙˆØ§Ø¨Ø· ÙØ±ÙŠØ¯Ø©: {unique_urls}")
            print(f"   ğŸ·ï¸  Ø¹Ø¯Ø¯ Ø§Ù„ÙØ¦Ø§Øª: {categories_count}")
            
            if total_records > 1000:
                issues.append("âš ï¸  Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ù…Ø¹Ø±ÙØ© ÙƒØ¨ÙŠØ±Ø© - Ù‚Ø¯ ØªØ­ØªØ§Ø¬ ØªÙ†Ø¸ÙŠÙ")
            
        except Exception as e:
            issues.append(f"âŒ Ø®Ø·Ø£ ÙÙŠ ÙØ­Øµ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª: {e}")
        
        # ÙØ­Øµ Ø§ØªØµØ§Ù„ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª
        try:
            response = requests.get("https://www.google.com", timeout=5)
            print("âœ… Ø§ØªØµØ§Ù„ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª: Ù†Ø´Ø·")
        except:
            issues.append("âŒ Ù…Ø´ÙƒÙ„Ø© ÙÙŠ Ø§ØªØµØ§Ù„ Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª")
        
        return issues
    
    def generate_health_report(self):
        """ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± ØµØ­Ø© Ø§Ù„Ø²Ø­Ù"""
        print("ğŸ“‹ ØªÙˆÙ„ÙŠØ¯ ØªÙ‚Ø±ÙŠØ± Ø§Ù„ØµØ­Ø©...")
        
        issues = self.analyze_crawler_issues()
        
        report = {
            "timestamp": datetime.now().isoformat(),
            "issues_found": len(issues),
            "issues": issues,
            "recommendations": [
                "Ø§Ø³ØªØ®Ø¯Ù… OptimizedWebSpider Ù„Ù„Ø²Ø­Ù Ø§Ù„Ø¢Ù…Ù†",
                "Ø­Ø¯Ø¯ max_depth Ø¥Ù„Ù‰ 2 Ø£Ùˆ 3",
                "Ø§Ø³ØªØ®Ø¯Ù… delay=1 Ø¨ÙŠÙ† Ø§Ù„Ø·Ù„Ø¨Ø§Øª",
                "Ø±Ø§Ù‚Ø¨ Ø­Ø¬Ù… Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ù†ØªØ¸Ø§Ù…"
            ]
        }
        
        # Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        os.makedirs("reports/diagnostics", exist_ok=True)
        report_file = "reports/diagnostics/crawler_health_report.json"
        
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"âœ… ØªÙ… Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±: {report_file}")
        
        # Ø¹Ø±Ø¶ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        print("\nğŸ¯ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„ØªØ­Ù„ÙŠÙ„:")
        if issues:
            for issue in issues:
                print(f"   {issue}")
        else:
            print("   âœ… Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø´Ø§ÙƒÙ„ Ø­Ø±Ø¬Ø©")

if __name__ == "__main__":
    fixer = CrawlerFix()
    fixer.generate_health_report()
